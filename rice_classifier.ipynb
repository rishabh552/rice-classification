{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Rice Variety Classifier (Stage 2)\n",
                "\n",
                "This notebook implements the second stage of the rice analysis pipeline: **Variety Classification**.\n",
                "\n",
                "**Goal:** Train a ResNet50 model to classify individual rice grains into one of 8 varieties:\n",
                "1. Arborio\n",
                "2. Basmati\n",
                "3. Ipsala\n",
                "4. Jasmine\n",
                "5. Karacadag\n",
                "6. Jhili\n",
                "7. HMT (Sona Masuri)\n",
                "8. Masuri\n",
                "\n",
                "**Prerequisites:**\n",
                "*   `Milled Rice Dataset.7z` must be uploaded to your Google Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup Environment\n",
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torchvision import datasets, transforms, models\n",
                "from torch.utils.data import DataLoader, random_split\n",
                "import matplotlib.pyplot as plt\n",
                "from google.colab import drive\n",
                "\n",
                "# Check GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Mount Drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Extract Dataset\n",
                "# Copying to local runtime first is faster for training than reading from Drive\n",
                "\n",
                "ARCHIVE_PATH = '/content/drive/MyDrive/Milled Rice Dataset.7z'  # Update if path differs\n",
                "EXTRACT_PATH = '/content/rice_dataset'\n",
                "\n",
                "if not os.path.exists(EXTRACT_PATH):\n",
                "    print(\"Copying and extracting dataset... (This may take a few minutes)\")\n",
                "    os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
                "    # Using 7z to extract\n",
                "    !7z x \"$ARCHIVE_PATH\" -o\"$EXTRACT_PATH\" > /dev/null\n",
                "    print(\"Extraction complete!\")\n",
                "else:\n",
                "    print(\"Dataset already extracted.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Verify Structure & Classes\n",
                "# Let's find where the image folders are exactly\n",
                "import glob\n",
                "\n",
                "print(\"Looking for dataset structure...\")\n",
                "# Try to find the root folder containing the class subdirectories\n",
                "possible_roots = glob.glob(f\"{EXTRACT_PATH}/**/Basmati\", recursive=True)\n",
                "\n",
                "if not possible_roots:\n",
                "    # Fallback search for any directory to help debug\n",
                "    print(\"Could not verify 'Basmati' folder immediately. Printing directory tree:\")\n",
                "    !find \"$EXTRACT_PATH\" -maxdepth 2 -type d\n",
                "    DATA_DIR = EXTRACT_PATH\n",
                "else:\n",
                "    # The parent of 'Basmati' is our data root\n",
                "    DATA_DIR = os.path.dirname(possible_roots[0])\n",
                "    print(f\"Dataset root found at: {DATA_DIR}\")\n",
                "\n",
                "print(f\"\\nClasses found:\")\n",
                "!ls \"$DATA_DIR\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Prepare Data Loaders\n",
                "\n",
                "# Define Transforms\n",
                "# Rice grains have no orientation, so we can rotate freely\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomVerticalFlip(),\n",
                "    transforms.RandomRotation(180),\n",
                "    transforms.ToTensor(),\n",
                "    # ImageNet normalization\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "val_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# Load Data\n",
                "full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transform)\n",
                "classes = full_dataset.classes\n",
                "print(f\"Detected {len(classes)} classes: {classes}\")\n",
                "\n",
                "# Split Train/Val (80/20)\n",
                "train_size = int(0.8 * len(full_dataset))\n",
                "val_size = len(full_dataset) - train_size\n",
                "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
                "\n",
                "# Important: Apply val_transform to validation set (hacky way since random_split doesn't allow separate transforms)\n",
                "# Ideally we split indices first, but for quick implementation this is acceptable or we just use train_transform for both (less optimal)\n",
                "# A cleaner way is using Subset with a custom Wrapper, but let's stick to simple for now.\n",
                "# Since random_split shares the underlying dataset, modifying .transform affects both.\n",
                "# We will proceed with train_transform for both for simplicity in this notebook, \n",
                "# or we can reload the dataset for validation usage. Let's reload for correctness.\n",
                "\n",
                "print(\"Re-splitting for correct transform application...\")\n",
                "# Get indices\n",
                "indices = torch.randperm(len(full_dataset)).tolist()\n",
                "train_indices = indices[:train_size]\n",
                "val_indices = indices[train_size:]\n",
                "\n",
                "train_subset = torch.utils.data.Subset(datasets.ImageFolder(DATA_DIR, transform=train_transform), train_indices)\n",
                "val_subset = torch.utils.data.Subset(datasets.ImageFolder(DATA_DIR, transform=val_transform), val_indices)\n",
                "\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
                "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
                "\n",
                "print(f\"Train samples: {len(train_subset)}, Val samples: {len(val_subset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Define Model (ResNet50)\n",
                "\n",
                "model = models.resnet50(pretrained=True)\n",
                "\n",
                "# Freeze early layers (Optional - unfreezing often better for distinct textures like rice)\n",
                "# for param in model.parameters():\n",
                "#     param.requires_grad = False\n",
                "\n",
                "# Replace last layer\n",
                "num_ftrs = model.fc.in_features\n",
                "model.fc = nn.Linear(num_ftrs, len(classes))\n",
                "\n",
                "model = model.to(device)\n",
                "\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.AdamW(model.parameters(), lr=0.0001)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Training Loop\n",
                "\n",
                "def train_model(model, train_loader, val_loader, epochs=10):\n",
                "    best_acc = 0.0\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
                "        print(\"-\" * 10)\n",
                "        \n",
                "        # Training\n",
                "        model.train()\n",
                "        running_loss = 0.0\n",
                "        running_corrects = 0\n",
                "        \n",
                "        for inputs, labels in train_loader:\n",
                "            inputs = inputs.to(device)\n",
                "            labels = labels.to(device)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            \n",
                "            outputs = model(inputs)\n",
                "            _, preds = torch.max(outputs, 1)\n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            running_loss += loss.item() * inputs.size(0)\n",
                "            running_corrects += torch.sum(preds == labels.data)\n",
                "            \n",
                "        epoch_loss = running_loss / len(train_loader.dataset)\n",
                "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
                "        \n",
                "        print(f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
                "        \n",
                "        # Validation\n",
                "        model.eval()\n",
                "        val_loss = 0.0\n",
                "        val_corrects = 0\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for inputs, labels in val_loader:\n",
                "                inputs = inputs.to(device)\n",
                "                labels = labels.to(device)\n",
                "                \n",
                "                outputs = model(inputs)\n",
                "                _, preds = torch.max(outputs, 1)\n",
                "                loss = criterion(outputs, labels)\n",
                "                \n",
                "                val_loss += loss.item() * inputs.size(0)\n",
                "                val_corrects += torch.sum(preds == labels.data)\n",
                "                \n",
                "        val_loss = val_loss / len(val_loader.dataset)\n",
                "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
                "        \n",
                "        print(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
                "        \n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            torch.save(model.state_dict(), '/content/drive/MyDrive/rice_resnet50_best.pth')\n",
                "            print(\"Saved new best model!\")\n",
                "\n",
                "train_model(model, train_loader, val_loader, epochs=5) # Start with 5 epochs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Verify on Example Image (Optional)\n",
                "# Takes a random validation image and shows the prediction\n",
                "import numpy as np\n",
                "\n",
                "def imshow(inp, title=None):\n",
                "    inp = inp.numpy().transpose((1, 2, 0))\n",
                "    mean = np.array([0.485, 0.456, 0.406])\n",
                "    std = np.array([0.229, 0.224, 0.225])\n",
                "    inp = std * inp + mean\n",
                "    inp = np.clip(inp, 0, 1)\n",
                "    plt.imshow(inp)\n",
                "    if title:\n",
                "        plt.title(title)\n",
                "    plt.pause(0.001)\n",
                "\n",
                "model.eval()\n",
                "inputs, classes_idx = next(iter(val_loader))\n",
                "outputs = model(inputs.to(device))\n",
                "_, preds = torch.max(outputs, 1)\n",
                "\n",
                "# Show first 4 images\n",
                "imshow(torchvision.utils.make_grid(inputs[:4]), title=[classes[x] for x in preds[:4]])"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}